#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æµ‹è¯•ä¿®å¤æ•ˆæœçš„è„šæœ¬
éªŒè¯æ‰€æœ‰ä¿®å¤é¡¹æ˜¯å¦ç”Ÿæ•ˆ
"""

import os
import re
import json
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))) if __name__ == "__main__" else sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

# æµ‹è¯•æ•°æ®ç›®å½•
TEST_MARKDOWN_DIR = "data/test_markdown"
TEST_IMAGES_DIR = "data/test_images"

# æµ‹è¯•URL
TEST_URL = "https://act.mihoyo.com/ys/ugc/tutorial/detail/mhogfq9bf86q"

# æ–‡æ¡£IDæ˜ å°„è¡¨
DOC_ID_MAP_FILE = "data/doc_id_map.json"

# åŠ è½½æ–‡æ¡£IDæ˜ å°„è¡¨
def load_doc_id_map():
    try:
        with open(DOC_ID_MAP_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"Failed to load doc_id_map: {e}")
        return {}

# æ‰§è¡Œå®Œæ•´çˆ¬è™«æµ‹è¯•
def run_crawler_test():
    """è¿è¡Œå®Œæ•´çš„çˆ¬è™«æµ‹è¯•ï¼ŒåŒ…æ‹¬çˆ¬å–ã€è§£æã€ä¸‹è½½å’Œé“¾æ¥æ›¿æ¢"""
    import sys
    import os
    
    # ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨Pythonè·¯å¾„ä¸­
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    
    from src.crawler.spider import Spider
    from src.crawler.parser import Parser
    from src.crawler.downloader import Downloader
    
    # åˆå§‹åŒ–çˆ¬è™«ç»„ä»¶
    spider = Spider()
    parser = Parser()
    downloader = Downloader()
    
    # åŠ è½½æ–‡æ¡£IDæ˜ å°„è¡¨
    doc_id_map = load_doc_id_map()
    
    # çˆ¬å–æµ‹è¯•é¡µé¢
    print(f"\næ­£åœ¨çˆ¬å–æµ‹è¯•é¡µé¢: {TEST_URL}")
    page_content = spider.get_page(TEST_URL)
    
    if not page_content:
        print("âœ— çˆ¬å–é¡µé¢å¤±è´¥")
        return None
    
    # æå–æ–‡æ¡£ID
    doc_id = TEST_URL.split('/')[-1]
    title = doc_id_map.get(doc_id, doc_id)
    
    # è§£æé¡µé¢å†…å®¹
    parsed_content = parser.parse_content(page_content)
    if not parsed_content:
        print("âœ— è§£æé¡µé¢å†…å®¹å¤±è´¥")
        return None
    
    # è§£æé¡µé¢ä¸­çš„å›¾ç‰‡
    images = parser.parse_images(page_content)
    print(f"âœ“ å‘ç° {len(images)} ä¸ªå›¾ç‰‡")
    
    # åœ¨æµ‹è¯•ç›®å½•ä¿å­˜Markdownæ–‡ä»¶
    test_filename = f"{title}_{doc_id[:8]}_test.md"
    test_filepath = os.path.join(TEST_MARKDOWN_DIR, test_filename)
    
    # ä¿å­˜åˆå§‹Markdownæ–‡ä»¶
    with open(test_filepath, 'w', encoding='utf-8') as f:
        f.write(parsed_content['content'])
    
    # ä¸‹è½½å›¾ç‰‡å¹¶è®°å½•æ˜ å°„å…³ç³»
    img_map = {}
    for img_url in images:
        downloaded_filename = downloader.download_image(img_url, TEST_IMAGES_DIR)
        if downloaded_filename:
            img_map[img_url] = downloaded_filename
            print(f"âœ“ ä¸‹è½½å›¾ç‰‡æˆåŠŸ: {downloaded_filename}")
        else:
            print(f"âœ— ä¸‹è½½å›¾ç‰‡å¤±è´¥: {img_url}")
    
    # è¯»å–ä¿å­˜çš„Markdownæ–‡ä»¶
    with open(test_filepath, 'r', encoding='utf-8') as f:
        markdown_content = f.read()
    
    # æ›¿æ¢å›¾ç‰‡é“¾æ¥
    if img_map:
        for img_url, local_filename in img_map.items():
            # åˆ›å»ºæœ¬åœ°ç›¸å¯¹è·¯å¾„
            local_path = f"../test_images/{local_filename}"
            # æ›¿æ¢CDNé“¾æ¥ä¸ºæœ¬åœ°ç›¸å¯¹è·¯å¾„
            markdown_content = markdown_content.replace(f"![]({img_url})", f"![]({local_path})")
            # æ›¿æ¢ç©ºå›¾ç‰‡é“¾æ¥
            markdown_content = markdown_content.replace("![]()", f"![]({local_path})")
    
    # è§£æé¡µé¢ä¸­çš„é“¾æ¥
    links = parser.parse_links(page_content, TEST_URL)
    print(f"âœ“ å‘ç° {len(links)} ä¸ªé“¾æ¥")
    
    # æ›¿æ¢æœ¬åœ°è·³è½¬é“¾æ¥
    if links:
        import re
        for link in links:
            # æå–æ–‡æ¡£ID
            link_doc_id = link.split('/')[-1]
            # æŸ¥æ‰¾å¯¹åº”çš„æœ¬åœ°æ–‡ä»¶å
            if link_doc_id in doc_id_map:
                link_title = doc_id_map[link_doc_id]
                local_filename = f"{link_title}_{link_doc_id[:8]}.md"
                # åˆ›å»ºæœ¬åœ°ç›¸å¯¹è·¯å¾„
                local_path = f"./{local_filename}"
                
                # æå–åŸºæœ¬URLéƒ¨åˆ†ï¼Œç”¨äºåŒ¹é…ä¸åŒæ ¼å¼çš„é“¾æ¥
                base_url = "/ys/ugc/tutorial/detail/"
                relative_link1 = f"/ys/ugc/tutorial//detail/{link_doc_id}"
                relative_link2 = f"{base_url}{link_doc_id}"
                
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢æ‰€æœ‰åŒ…å«è¯¥é“¾æ¥çš„Markdowné“¾æ¥ï¼Œæ— è®ºé“¾æ¥æ–‡æœ¬æ˜¯ä»€ä¹ˆ
                for url_pattern in [link, relative_link1, relative_link2]:
                    # è½¬ä¹‰URLä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œç”¨äºæ­£åˆ™è¡¨è¾¾å¼
                    escaped_url = re.escape(url_pattern)
                    # åŒ¹é…Markdowné“¾æ¥æ ¼å¼ï¼š[ä»»æ„æ–‡æœ¬](URL)
                    link_pattern = re.compile(r'\[([^\]]+)\]\(\s*' + escaped_url + r'\s*\)')
                    # æ›¿æ¢ä¸ºæœ¬åœ°é“¾æ¥
                    markdown_content = link_pattern.sub(rf'[\1]({local_path})', markdown_content)
    
    # ä¿å­˜æ›´æ–°åçš„Markdownæ–‡ä»¶
    with open(test_filepath, 'w', encoding='utf-8') as f:
        f.write(markdown_content)
    
    print(f"âœ“ å®Œæ•´çˆ¬è™«æµç¨‹æµ‹è¯•å®Œæˆï¼Œæµ‹è¯•æ–‡ä»¶å·²ä¿å­˜åˆ°: {test_filepath}")
    
    # è¿”å›å¤„ç†åçš„å†…å®¹
    return markdown_content

# æµ‹è¯•ç©ºå›¾ç‰‡é“¾æ¥ä¿®å¤
def test_empty_image_links(content):
    """æµ‹è¯•ç©ºå›¾ç‰‡é“¾æ¥æ˜¯å¦å·²ä¿®å¤"""
    # æŸ¥æ‰¾æ‰€æœ‰ç©ºå›¾ç‰‡é“¾æ¥
    empty_img_pattern = r'!\[\]\(\)'
    empty_img_matches = re.findall(empty_img_pattern, content)
    
    if not empty_img_matches:
        return True, "âœ“ ç©ºå›¾ç‰‡é“¾æ¥å·²ä¿®å¤ï¼Œæœªå‘ç°ç©ºå›¾ç‰‡é“¾æ¥"
    else:
        return False, f"âœ— ä»å­˜åœ¨ç©ºå›¾ç‰‡é“¾æ¥ï¼Œå…± {len(empty_img_matches)} ä¸ª"

# æµ‹è¯•è‡ªå®šä¹‰åˆ—è¡¨ä¿®å¤
def test_custom_lists(content):
    """æµ‹è¯•è‡ªå®šä¹‰åˆ—è¡¨æ˜¯å¦å·²ä¿®å¤"""
    # æŸ¥æ‰¾æ‰€æœ‰è‡ªå®šä¹‰åˆ—è¡¨æ ‡è®°
    custom_list_marker = r'îšœ'
    custom_list_matches = re.findall(custom_list_marker, content)
    
    if not custom_list_matches:
        # æŸ¥æ‰¾æ ‡å‡†Markdownåˆ—è¡¨
        standard_list_pattern = r'^[\s]*[-*+]\s+'
        standard_list_matches = re.findall(standard_list_pattern, content, re.MULTILINE)
        if standard_list_matches:
            return True, f"âœ“ è‡ªå®šä¹‰åˆ—è¡¨å·²ä¿®å¤ï¼Œå…±å‘ç° {len(standard_list_matches)} ä¸ªæ ‡å‡†åˆ—è¡¨é¡¹"
        else:
            return True, "âœ“ è‡ªå®šä¹‰åˆ—è¡¨å·²ä¿®å¤ï¼Œæœªå‘ç°è‡ªå®šä¹‰åˆ—è¡¨æ ‡è®°"
    else:
        return False, f"âœ— ä»å­˜åœ¨è‡ªå®šä¹‰åˆ—è¡¨æ ‡è®°ï¼Œå…± {len(custom_list_matches)} ä¸ª"

# æµ‹è¯•å›¾ç‰‡é“¾æ¥æœ¬åœ°åŒ–
def test_image_link_localization(content):
    """æµ‹è¯•å›¾ç‰‡é“¾æ¥æ˜¯å¦å·²è½¬æ¢ä¸ºæœ¬åœ°è·¯å¾„"""
    # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡é“¾æ¥
    img_pattern = r'!\[.*?\]\((.*?)\)'
    img_matches = re.findall(img_pattern, content)
    
    cdn_links = []
    local_links = []
    
    for img_url in img_matches:
        if img_url.startswith("http"):
            cdn_links.append(img_url)
        elif img_url.startswith("../images/"):
            local_links.append(img_url)
    
    if not cdn_links:
        return True, f"âœ“ å›¾ç‰‡é“¾æ¥å·²æœ¬åœ°åŒ–ï¼Œå…± {len(local_links)} ä¸ªæœ¬åœ°å›¾ç‰‡é“¾æ¥"
    else:
        return False, f"âœ— ä»å­˜åœ¨CDNå›¾ç‰‡é“¾æ¥ï¼Œå…± {len(cdn_links)} ä¸ªCDNé“¾æ¥ï¼Œ{len(local_links)} ä¸ªæœ¬åœ°é“¾æ¥"

# æµ‹è¯•é“¾æ¥æœ¬åœ°åŒ–
def test_link_localization(content, doc_id_map):
    """æµ‹è¯•é“¾æ¥æ˜¯å¦å·²è½¬æ¢ä¸ºæœ¬åœ°æ–‡ä»¶è·¯å¾„"""
    # æŸ¥æ‰¾æ‰€æœ‰Markdowné“¾æ¥
    link_pattern = r'\[.*?\]\((.*?)\)'
    link_matches = re.findall(link_pattern, content)
    
    external_links = []
    local_links = []
    
    for link in link_matches:
        if link.startswith("http") or link.startswith("/ys/ugc/tutorial"):
            external_links.append(link)
        elif link.endswith(".md"):
            local_links.append(link)
    
    if not external_links:
        return True, f"âœ“ é“¾æ¥å·²æœ¬åœ°åŒ–ï¼Œå…± {len(local_links)} ä¸ªæœ¬åœ°é“¾æ¥"
    else:
        # æ‰“å°å…·ä½“çš„å¤–éƒ¨é“¾æ¥ï¼Œç”¨äºè°ƒè¯•
        print(f"\nè°ƒè¯•ä¿¡æ¯ï¼šæœªæœ¬åœ°åŒ–çš„å¤–éƒ¨é“¾æ¥ï¼š")
        for link in external_links:
            print(f"  - {link}")
        return False, f"âœ— ä»å­˜åœ¨å¤–éƒ¨é“¾æ¥ï¼Œå…± {len(external_links)} ä¸ªå¤–éƒ¨é“¾æ¥ï¼Œ{len(local_links)} ä¸ªæœ¬åœ°é“¾æ¥"

# ä¸»æµ‹è¯•å‡½æ•°
def main():
    print("=" * 60)
    print("æµ‹è¯•ä¿®å¤æ•ˆæœ")
    print("=" * 60)
    
    # è¿è¡Œçˆ¬è™«æµ‹è¯•ï¼Œè·å–ä¿®å¤åçš„å†…å®¹
    content = run_crawler_test()
    
    if not content:
        print("\nâŒ æ— æ³•è·å–æµ‹è¯•å†…å®¹ï¼Œæµ‹è¯•å¤±è´¥ã€‚")
        return False
    
    # åŠ è½½æ–‡æ¡£IDæ˜ å°„è¡¨
    doc_id_map = load_doc_id_map()
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("ç©ºå›¾ç‰‡é“¾æ¥ä¿®å¤", test_empty_image_links, [content]),
        ("è‡ªå®šä¹‰åˆ—è¡¨ä¿®å¤", test_custom_lists, [content]),
        ("å›¾ç‰‡é“¾æ¥æœ¬åœ°åŒ–", test_image_link_localization, [content]),
        ("é“¾æ¥æœ¬åœ°åŒ–", test_link_localization, [content, doc_id_map])
    ]
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    results = []
    all_passed = True
    
    for test_name, test_func, args in tests:
        print(f"\næµ‹è¯•é¡¹: {test_name}")
        print("-" * 40)
        
        success, message = test_func(*args)
        print(message)
        
        results.append({
            "test_name": test_name,
            "success": success,
            "message": message
        })
        
        if not success:
            all_passed = False
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    for result in results:
        status = "âœ“" if result["success"] else "âœ—"
        print(f"{status} {result['test_name']}: {result['message']}")
    
    print("\n" + "=" * 60)
    print("æœ€ç»ˆç»“è®º")
    print("=" * 60)
    
    # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
    save_test_report(results, all_passed, content)
    
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰ä¿®å¤é¡¹å‡å·²é€šè¿‡æµ‹è¯•ï¼")
    else:
        print("âŒ éƒ¨åˆ†ä¿®å¤é¡¹æœªé€šè¿‡æµ‹è¯•ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
    
    return all_passed

# ä¿å­˜æµ‹è¯•æŠ¥å‘Š
def save_test_report(results, all_passed, content):
    """ä¿å­˜æµ‹è¯•æŠ¥å‘Šåˆ°æ–‡ä»¶"""
    from datetime import datetime
    
    # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶å
    report_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"test_report_{report_time}.md"
    
    # ç”ŸæˆæŠ¥å‘Šå†…å®¹
    report = f"# ä¿®å¤æ•ˆæœæµ‹è¯•æŠ¥å‘Š\n\n"
    report += f"## æµ‹è¯•åŸºæœ¬ä¿¡æ¯\n"
    report += f"- æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
    report += f"- æµ‹è¯•URL: {TEST_URL}\n"
    report += f"- æµ‹è¯•ç»“æœ: {'é€šè¿‡' if all_passed else 'æœªé€šè¿‡'}\n\n"
    
    report += f"## æµ‹è¯•é¡¹ç»“æœ\n"
    for result in results:
        status = "âœ…" if result["success"] else "âŒ"
        report += f"- {status} {result['test_name']}: {result['message']}\n"
    
    report += f"\n## æµ‹è¯•å†…å®¹æ‘˜è¦\n"
    # æå–å‰500ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦
    content_summary = content[:500] + "..." if len(content) > 500 else content
    report += f"```markdown\n{content_summary}\n```\n"
    
    # ä¿å­˜æŠ¥å‘Š
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

if __name__ == "__main__":
    main()
