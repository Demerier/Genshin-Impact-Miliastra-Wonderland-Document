# 启动爬虫计划

## 1. 确认准备工作

### 1.1 配置检查
- [x] 爬取配置文件（crawl_config.py）已就绪
- [x] 请求延迟设置：2.0-3.0秒
- [x] 最大重试次数：5次
- [x] 超时时间：30秒
- [x] 增量更新：启用
- [x] 断点续传：启用

### 1.2 页面列表
当前包含3个测试页面：
1. 教程列表页面
2. 整体界面（7张图片）
3. 文档页面（蓝色斜体文本）

### 1.3 存储路径
- Markdown目录：`data/markdown`
- HTML目录：`data/html`
- 图片目录：`data/images`
- 日志目录：`logs/crawl`

### 1.4 工作留痕机制
- 日志记录：详细的爬取日志，包含时间戳、级别、文件名、行号
- 统计报告：JSON格式的统计信息，包含配置参数和爬取结果
- 断点续传：记录已完成和失败的页面，支持中断后继续
- 文件命名：Markdown和图片文件使用有意义的命名规则

## 2. 启动爬虫步骤

### 2.1 执行启动脚本
```bash
python run_crawl.py
```

### 2.2 启动过程
1. 显示爬取配置信息
2. 显示爬取页面列表
3. 显示存储路径
4. 等待用户确认（按Enter键）
5. 启动爬取器
6. 执行爬取任务
7. 生成统计报告

### 2.3 监控重点
- 日志输出：检查是否有错误信息
- 进度显示：确认爬取进度正常
- 资源使用：监控内存和CPU使用情况

## 3. 验证工作留痕

### 3.1 日志文件
- 检查`logs/crawl/`目录下的日志文件
- 确认日志包含完整的爬取过程
- 确认日志包含错误信息（如果有）

### 3.2 统计报告
- 检查`logs/crawl/`目录下的统计文件（crawl_stats_*.json）
- 确认统计信息完整，包括：
  - 总页面数、成功页面、失败页面、跳过页面
  - 总加载时间、平均加载时间
  - 总内容长度、平均内容长度
  - 下载图片总数、提取链接总数

### 3.3 断点续传文件
- 检查`data/crawl_resume.json`文件
- 确认包含已完成和失败的页面列表

### 3.4 生成的文件
- 检查`data/markdown/`目录下的Markdown文件
- 检查`data/images/`目录下的图片文件
- 确认文件名符合预期格式

## 4. 预期结果

### 4.1 爬取成功
- 所有页面爬取成功
- 生成对应的Markdown文件
- 下载所有图片
- 日志记录完整
- 统计报告生成成功

### 4.2 部分失败
- 失败页面记录在断点续传文件中
- 日志包含详细的错误信息
- 统计报告反映真实情况

## 5. 后续操作

### 5.1 分析爬取结果
- 检查生成的Markdown文件质量
- 分析图片下载情况
- 检查链接转换是否正确

### 5.2 调整配置（如果需要）
- 根据爬取结果调整请求延迟
- 调整重试次数
- 扩展页面列表

### 5.3 正式爬取
- 在测试爬取成功后，扩展页面列表
- 执行正式爬取任务
- 定期检查爬取结果和日志